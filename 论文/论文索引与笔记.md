## 1.Aesthetic image cropping meets VLP Enhancing good while reducing bad 美学图像裁剪与 VLP 相遇：提升优点，减少缺点 2024

 [1-s2.0-S1047320324002724-main.pdf](1-s2.0-S1047320324002724-main.pdf) 

本篇文章主要介绍了一种名为CLIPCropping的模型，对图像进行基于美学的图像裁剪

美观图像裁剪（AIC）通过调整图像的构图和美学元素来增强其视觉效果。人们根据这些元素进行调整，旨在增强吸引人的方面，同时最小化不利因素。受这些观察结果的启发，我们提出了一种名为 CLIPCropping 的新方法，该方法模拟了 AIC 中的人类决策过程。CLIPCropping 利用对比语言-图像预训练（CLIP）将视觉感知与文本描述进行对齐。它由三个分支组成：构图嵌入、美学嵌入和图像裁剪。组合嵌入分支基于组合知识嵌入（CKE）学习原则，而美学嵌入分支基于美学知识嵌入（AKE）学习原则。图像裁剪分支通过聚合 CKE 和 AKE 的知识来评估候选裁剪的质量；一个 MLP 产生最佳结果。在 GAICD-1236、GAICD-3336 和 FCDB 三个基准数据集上的大量实验表明，CLIPCropping 优于当前最先进的方法，并提供有见地的解释。

## 2.**ProCrop: Learning Aesthetic Image Cropping from Professional Compositions** 2025

 [2505.22490v1.pdf](2505.22490v1.pdf) 

**这个项目是比较有意思的，也是唯一一个比较新的论文实现方法**

<img src="E:\LiveCompose\论文\assets\image-20250704142319273.png" alt="image-20250704142319273"  />

论文使用摄影师的摄影作品，经过SAM

> 在本文中，我们介绍了一种基于检索的图像裁剪方法，该方法利用了现有专业摄影的丰富资源。受语言模型中的检索增强[4, 12]以及专业摄影数据集的丰富性的启发，我们从具有相似美学构图的职业图像中学习（见图 1）。我们的关键洞察是，专业摄影师已经通过他们的经验和艺术视野解决了许多构图挑战。通过利用这些知识库，我们引导模型与专业标准保持一致。
>
> 此外，我们通过开发一种弱监督方法的大规模数据集来解决高质量美学训练数据的稀缺问题。具体来说，我们利用 ControlNet [56]（一种文本到图像扩散模型）来扩展专业图像，模拟裁剪和未裁剪的图像对。从 AVA [31] 和 unsplash-lite [43] 开始，这些作为专家标签（即好的裁剪）的大型专业图像集合，我们使用 GPT-4 [1] 推断原始图像边界之外的文本布局，并使用 SAM [19] 提取多尺度构图掩码。这些掩码随后被输入到 ControlNet 中进行图像扩展。通过迭代优化过程，我们生成了多样化的裁剪建议，显著扩展了可用数据。

该论文介绍了 ProCrop，一种基于检索的方法，用于学习图像的美学构图裁剪。现有方法通常缺乏多样性或需要大量标注数据。ProCrop 通过利用专业摄影作品来指导裁剪决策，将专业照片的特征与查询图像的特征融合，从而学习专业构图，显著提高了性能。
此外，该论文提出了一个大型弱标注数据集，包含 24.2 万张图像。该数据集是通过对专业图像进行 outpainting，并迭代优化多样化的裁剪建议而生成的。这种构图感知的数据集生成方法提供了由美学原则指导的多样化高质量裁剪建议，并且是目前公开可用的最大图像裁剪数据集。

## 3.**Hong_Composing_Photos_Like_a_Photographer** 2021

 [Hong_Composing_Photos_Like_a_Photographer_CVPR_2021_paper.pdf](Hong_Composing_Photos_Like_a_Photographer_CVPR_2021_paper.pdf) 

本篇论文是华为CBG的作品

> 我们证明了，清晰的构图规则建模有利于图像裁剪。图像裁剪被认为是专业摄影中实现美学构图自动化的一种有前景的方法。
> 然而，现有的努力只是隐式地模拟这种专业知识，例如通过从比较候选人中进行排名。
> 受到自然组成特征始终遵循特定规则的观察结果的启发，我们建议以判别的方式学习这些规则，更重要的是，将学习到的组成线索明确地纳入模型中。
> 为此，我们引入了关键构图图（KCM）的概念来编码构图规则。KCM 可以揭示不同构图规则背后隐藏的共同规律，并告知裁剪模型构图过程中哪些部分是重要的。
> 基于 KCM，我们提出了一种新颖的按构图裁剪范式，并实例化了一个网络来实现构图感知的图像裁剪。在两个基准测试上进行的大量实验证明了我们的方法能够实现高效、可解释且快速的图像裁剪。

## 4.**Rethinking Image Cropping: Exploring Diverse Compositions from Global Views**

[Jia_Rethinking_Image_Cropping_Exploring_Diverse_Compositions_From_Global_Views_CVPR_2022_paper.pdf](Jia_Rethinking_Image_Cropping_Exploring_Diverse_Compositions_From_Global_Views_CVPR_2022_paper.pdf) 

Existing image cropping works mainly use anchor evaluation methods or coordinate regression methods. However, it is difficult  for pre-defined anchors to cover good crops globally, and the regression methods ignore the cropping diversity.
现有的图像裁剪工作主要使用锚点评估方法或坐标回归方法。然而，预定义的锚点难以全面覆盖良好的全局裁剪，而回归方法忽略了裁剪的多样性。
In this paper, we regard image cropping as a set prediction problem. A set of crops regressed from multiple learnable anchors is matched with the labeled good crops, and a classifier is trained using the matching results to select a valid subset from all the predictions.
在本文中，我们将图像裁剪视为一个集合预测问题。从多个可学习的锚点回归得到的裁剪集合与标记的优质裁剪进行匹配，并使用匹配结果训练分类器，从所有预测中选择一个有效的子集。
This new perspective equips our model with globality and diversity, mitigating the shortcomings but inherit the strengthens of previous methods. Despite the advantages, the set prediction method causes inconsistency between the validity labels and the crops.
这种新的视角使我们的模型具有全局性和多样性，弥补了先前方法的不足，同时继承了其优势。尽管具有这些优势，集合预测方法导致有效性标签与裁剪之间存在不一致性。
To deal with this problem, we propose to smooth the validity labels with two different methods. The first method that uses crop qualities as direct guidance is designed for the datasets with nearly dense quality labels.
为解决这一问题，我们提出使用两种不同的方法来平滑有效性标签。第一种方法使用裁剪质量作为直接指导，适用于几乎具有密集质量标签的数据集。
The second method based on the self distillation can be used in sparsely labeled datasets. Experimental results on the public datasets show the merits of our approach over state-of-the-art counterparts.
第二种基于自蒸馏的方法可用于稀疏标签的数据集。在公共数据集上的实验结果表明，我们的方法优于最先进的方法。

## 5.Multi-Modality Multi-Attribute Contrastive Pre-Training for Image Aesthetics Computing

 [Multi-Modality_Multi-Attribute_Contrastive_Pre-Training_for_Image_Aesthetics_Computing.pdf](Multi-Modality_Multi-Attribute_Contrastive_Pre-Training_for_Image_Aesthetics_Computing.pdf) 

In the Image Aesthetics Computing (IAC) field, most prior methods leveraged the off-the-shelf backbones pre-trained on the large-scale ImageNet database.
在图像美学计算（IAC）领域，大多数现有方法利用在大型 ImageNet 数据库上预训练的现成骨干网络。
While these pre-trained backbones have achieved notable success, they often overemphasize object-level semantics and fail to capture the high-level concepts of image aesthetics, which may only achieve suboptimal performances.
虽然这些预训练的骨干网络取得了显著的成功，但它们往往过度强调对象级语义，而无法捕捉图像美学的宏观概念，这可能导致性能欠佳。
To tackle this long-neglected problem, we propose a multi-modality multi-attribute contrastive pre-training framework, targeting at constructing an alternative to ImageNet-based pre-training for IAC. Specifically, the proposed framework consists of two main aspects.
为了解决这个长期被忽视的问题，我们提出了一种多模态多属性对比预训练框架，旨在构建一个替代基于 ImageNet 的 IAC 预训练方案。具体而言，所提出的框架包含两个主要方面。

1) We build a multi-attribute image description database with human feedback, leveraging the competent image understanding capability of the multi-modality large language model to generate rich aesthetic descriptions.
2) 我们构建了一个具有人类反馈的多属性图像描述数据库，利用多模态大语言模型强大的图像理解能力来生成丰富的美学描述。
3) To better adapt models to aesthetic computing tasks, we integrate the image-based visual features with the attribute-based text features, and map the integrated features into different embedding spaces, based on which the multi-attribute contrastive learning is proposed for obtaining more comprehensive aesthetic representation.
4) 为了更好地使模型适应美学计算任务，我们将基于图像的视觉特征与基于属性的文本特征进行整合，并将整合后的特征映射到不同的嵌入空间中，基于此提出了多属性对比学习，以获得更全面的美学表示。

To alleviate the distribution shift encountered when transitioning from the general visual domain to the aesthetic domain, we further propose a semantic affinity loss to restrain the content information and enhance model generalization.
为了缓解从通用视觉域过渡到美学域时遇到的分布偏移问题，我们进一步提出了一种语义亲和损失函数，以约束内容信息并增强模型的泛化能力。
Extensive experiments demonstrate that the proposed framework sets new state-of-the-arts for IAC tasks.
大量实验表明，所提出的框架为 IAC 任务设定了新的 SOTA 标准。

## 6.Cropper: Vision-Language Model for Image Cropping through In-Context Learning

 [2408.07790v2.pdf](2408.07790v2.pdf) 

The goal of image cropping is to identify visually appealing crops in an image. Conventional methods are trained on specific datasets and fail to adapt to new requirements.
图像裁剪的目标是在图像中识别出视觉上吸引人的裁剪区域。传统方法是在特定数据集上训练的，无法适应新的需求。
Recent breakthroughs in large vision-language models (VLMs) enable visual in-context learning without explicit training. However, downstream tasks with VLMs remain under explored. In this paper, we propose an effective approach to leverage VLMs for image cropping.
大型视觉语言模型（VLMs）的最新突破使得无需显式训练即可进行视觉情境学习。然而，使用 VLMs 的下游任务仍需进一步探索。在本文中，我们提出了一种有效的方法，利用 VLMs 进行图像裁剪。
First, we propose an efficient prompt retrieval mechanism for image cropping to automate the selection of in-context examples. Second, we introduce an iterative refinement strategy to iteratively enhance the predicted crops.
首先，我们提出了一种针对图像裁剪的高效提示检索机制，以自动化选择情境示例。其次，我们引入了一种迭代优化策略，以迭代增强预测的裁剪区域。
The proposed framework, we refer to as Cropper, is applicable to a wide range of cropping tasks, including free-form cropping, subject-aware cropping, and aspect ratio-aware cropping.
我们提出的框架，称为 Cropper，适用于多种裁剪任务，包括自由形式裁剪、主体感知裁剪和宽高比感知裁剪。
Extensive experiments demonstrate that Cropper significantly outperforms state-of-the-art methods across several benchmarks.
大量实验表明，Cropper 在多个基准测试中显著优于现有最优方法。